"""
File: ragchain.py
Description: [Brief description of the file's purpose]
Author: Arturo Gomez-Chavez
Creation Date: 07.07.2025
Institution/Organization: NA
Contributors/Editors:
License: MIT License - See LICENSE.MD file for details
Contact & Support:
- Email: [support@example.com]
"""

"""
ragchain.py - Retrieval-Augmented Generation (RAG) chain implementation

This module contains the core RAG functionality:
- Setting up the retrieval chain to find relevant documents
- Creating prompts that combine context with user questions
- Generating responses using the language model
- Managing the complete RAG workflow
"""

import streamlit as st
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from utils import initialize_models, format_docs


def validate_temperature(temperature):
    """
    Validate temperature parameter for language model.
    
    Args:
        temperature (float): Temperature value to validate
    
    Returns:
        tuple: (is_valid, error_message, normalized_value)
    """
    try:
        temp_float = float(temperature)
        
        # Check range - Gemini accepts 0.0 to 2.0
        if temp_float < 0.0:
            return False, "Temperature must be 0.0 or higher", 0.0
        elif temp_float > 2.0:
            return False, "Temperature must be 2.0 or lower", 2.0
        else:
            return True, "Valid temperature", temp_float
            
    except (ValueError, TypeError):
        return False, "Temperature must be a valid number", 1.0


def get_temperature_description(temperature):
    """
    Get human-readable description of temperature effects.
    
    Args:
        temperature (float): Temperature value
    
    Returns:
        str: Description of what this temperature setting does
    """
    if temperature == 0.0:
        return "üéØ **Deterministic**: Same answer every time, most factual and focused"
    elif temperature <= 0.3:
        return "üìä **Very Conservative**: Highly consistent, minimal creativity"
    elif temperature <= 0.7:
        return "‚öñÔ∏è **Balanced**: Good mix of consistency and natural variation"
    elif temperature <= 1.0:
        return "üé® **Creative**: More varied responses, natural conversational tone"
    elif temperature <= 1.5:
        return "üöÄ **Highly Creative**: Very diverse responses, more experimental"
    else:
        return "üé≤ **Maximum Creativity**: Highly unpredictable, very experimental responses"


def run_rag_chain(query, custom_prompt=None, temperature=1.0):
    """
    Processes a user query using a Retrieval-Augmented Generation (RAG) chain.
    
    The RAG process works in these steps:
    1. **Retrieval**: Find relevant document chunks using similarity search
    2. **Augmentation**: Combine retrieved context with the user's question
    3. **Generation**: Use a language model to generate an answer based on context
    
    This approach ensures that:
    - Answers are grounded in the uploaded documents
    - The model has access to specific, relevant information
    - Responses are more accurate and factual than general knowledge alone
    
    Args:
        query (str): The user's question that needs to be answered
        custom_prompt (str, optional): Custom prompt template to use instead of default.
                                     Must include {context} and {question} placeholders.
        temperature (float, optional): Controls randomness in responses (0.0-2.0).
                                     0.0 = deterministic, 1.0 = balanced, 2.0 = very creative.
                                     Default is 1.0.
    
    Returns:
        str: A response generated by the language model based on retrieved context,
             or an error message if something goes wrong.
    
    Example:
        >>> response = run_rag_chain("What are the effects of climate change?", temperature=0.7)
        >>> print(response)
        "Based on the documents, climate change causes rising sea levels..."
    """
    
    # Step 1: Validate prerequisites
    if not st.session_state.get("gemini_api_key"):
        st.error("‚ùå Please enter your Gemini API key first!")
        return "API key required to process queries."

    # Step 2: Validate temperature parameter
    is_valid, error_msg, normalized_temp = validate_temperature(temperature)
    if not is_valid:
        st.warning(f"‚ö†Ô∏è Temperature issue: {error_msg}. Using {normalized_temp} instead.")
        temperature = normalized_temp

    # Step 3: Initialize models (embedding model and vector database)
    initialize_models()
    
    # Step 4: Create retriever for finding relevant documents
    # The retriever uses similarity search to find the most relevant chunks
    retriever = st.session_state.db.as_retriever(
        search_type="similarity",     # Use cosine similarity between embeddings
        search_kwargs={'k': 5}        # Retrieve top 5 most similar chunks
    )

    # Step 5: Set up the prompt template
    if custom_prompt:
        PROMPT_TEMPLATE = custom_prompt
    else:
        # Default prompt template optimized for Environmental sciences
        # You can modify this to match your specific domain
        PROMPT_TEMPLATE = """You are a highly knowledgeable assistant specializing in Environmental sciences. 
Answer the question based only on the following context:
{context}

Answer the question based on the above context:
{question}

Use the provided context to answer the user's question accurately and concisely.
Justify your answers.
Don't give information not mentioned in the CONTEXT INFORMATION.
Do not say "according to the context" or "mentioned in the context" or similar."""

    # Create the prompt template object
    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)

    # Step 6: Initialize the language model for generation with specified temperature
    chat_model = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",                              # Google's fast, efficient model
        api_key=st.session_state.get("gemini_api_key"),
        temperature=temperature                                # User-controlled randomness parameter
    )

    # Step 7: Set up output parser to extract text from model response
    output_parser = StrOutputParser()

    # Step 8: Build the RAG chain using LangChain's pipe operator (|)
    # This creates a pipeline: retrieve ‚Üí format ‚Üí prompt ‚Üí generate ‚Üí parse
    rag_chain = (
        {
            "context": retriever | format_docs,    # Retrieve docs and format as single string
            "question": RunnablePassthrough()      # Pass the question through unchanged
        } 
        | prompt_template                          # Insert context and question into prompt
        | chat_model                               # Generate response using language model
        | output_parser                            # Extract text from model response
    )

    # Step 9: Execute the RAG chain with the user's query
    try:
        response = rag_chain.invoke(query)
        return response
    except Exception as e:
        st.error(f"‚ùå Error processing query: {str(e)}")
        return f"Sorry, I encountered an error while processing your query: {str(e)}"


def get_relevant_documents(query, k=5):
    """
    Retrieve relevant documents for a query without generating a response.
    
    This function is useful for:
    - Debugging retrieval quality
    - Understanding what context the model will use
    - Building custom RAG workflows
    
    Args:
        query (str): The query to search for
        k (int): Number of documents to retrieve (default: 5)
    
    Returns:
        list: List of relevant document objects, or empty list if error
    """
    try:
        # Ensure models are initialized
        if not st.session_state.get("db"):
            initialize_models()
        
        # Create retriever and get documents
        retriever = st.session_state.db.as_retriever(
            search_type="similarity",
            search_kwargs={'k': k}
        )
        
        docs = retriever.get_relevant_documents(query)
        return docs
        
    except Exception as e:
        st.error(f"‚ùå Error retrieving documents: {str(e)}")
        return []


def create_custom_rag_chain(prompt_template, model_params=None):
    """
    Create a custom RAG chain with specific parameters.
    
    This function allows for more advanced customization of the RAG process,
    including different model parameters and prompt structures.
    
    Args:
        prompt_template (str): Custom prompt template with {context} and {question}
        model_params (dict, optional): Parameters for the language model
                                     (temperature, max_tokens, etc.)
    
    Returns:
        RunnableSequence: Custom RAG chain ready for invocation
    """
    try:
        # Initialize models
        initialize_models()
        
        # Set up retriever
        retriever = st.session_state.db.as_retriever(
            search_type="similarity",
            search_kwargs={'k': 5}
        )
        
        # Create prompt template
        prompt = ChatPromptTemplate.from_template(prompt_template)
        
        # Set up model with custom parameters
        default_params = {
            "model": "gemini-2.5-flash",
            "api_key": st.session_state.get("gemini_api_key"),
            "temperature": 1.0
        }
        
        if model_params:
            # Validate temperature if provided
            if "temperature" in model_params:
                is_valid, error_msg, normalized_temp = validate_temperature(model_params["temperature"])
                if not is_valid:
                    st.warning(f"‚ö†Ô∏è Temperature issue in custom chain: {error_msg}. Using {normalized_temp} instead.")
                    model_params["temperature"] = normalized_temp
            
            default_params.update(model_params)
        
        chat_model = ChatGoogleGenerativeAI(**default_params)
        output_parser = StrOutputParser()
        
        # Build custom chain
        custom_chain = (
            {
                "context": retriever | format_docs,
                "question": RunnablePassthrough()
            }
            | prompt
            | chat_model
            | output_parser
        )
        
        return custom_chain
        
    except Exception as e:
        st.error(f"‚ùå Error creating custom RAG chain: {str(e)}")
        return None


def validate_prompt_template(prompt_template):
    """
    Validate that a prompt template contains required placeholders.
    
    Args:
        prompt_template (str): The prompt template to validate
    
    Returns:
        tuple: (is_valid, error_message)
    """
    if not prompt_template:
        return False, "Prompt template cannot be empty"
    
    if "{context}" not in prompt_template:
        return False, "Prompt template must include {context} placeholder"
    
    if "{question}" not in prompt_template:
        return False, "Prompt template must include {question} placeholder"
    
    return True, "Prompt template is valid"